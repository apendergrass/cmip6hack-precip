{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "from tqdm.autonotebook import tqdm\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments to process\n",
    "experiment_ids = ['historical', 'ssp370']\n",
    "experiment_ids1 = ['historical']\n",
    "\n",
    "# Seasons to process\n",
    "seasons = ['all','DJF','JJA']\n",
    "\n",
    "# Time slices (future) to process\n",
    "time_slices = ([['1991','2020'],\n",
    "                ['2001','2030'],\n",
    "                ['2011','2040'],\n",
    "                ['2021','2050'],\n",
    "                ['2031','2060'],\n",
    "                ['2041','2070'],\n",
    "                ['2051','2080'],\n",
    "                ['2061','2090'],\n",
    "                ['2071','2100']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data locations\n",
    "df = pd.read_csv('https://storage.googleapis.com/pangeo-cmip6/pangeo-cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to monthly precipitation (let's just look at one run per model for now)\n",
    "df_mon_pr = (df[((df.table_id == 'Amon') \n",
    "                 & (df.variable_id == 'pr')\n",
    "                 & (df.member_id == \"r1i1p1f1\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of all the models in the subset\n",
    "pr_source_ids = []\n",
    "for name, group in df_mon_pr.groupby('source_id'):\n",
    "    if all([expt in group.experiment_id.values\n",
    "            for expt in experiment_ids]):\n",
    "        pr_source_ids.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to monthly tas (let's just look at one run per model for now)\n",
    "df_mon_tas = (df[((df.table_id == 'Amon') \n",
    "                 & (df.variable_id == 'tas')\n",
    "                 & (df.member_id == \"r1i1p1f1\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of all the models in the subset\n",
    "tas_source_ids = []\n",
    "for name, group in df_mon_tas.groupby('source_id'):\n",
    "    if all([expt in group.experiment_id.values\n",
    "            for expt in experiment_ids]):\n",
    "        tas_source_ids.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CESM2',\n",
       " 'CESM2-WACCM',\n",
       " 'CanESM5',\n",
       " 'E3SM-1-0',\n",
       " 'GFDL-CM4',\n",
       " 'GFDL-ESM4',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'MCM-UA-1-0',\n",
       " 'MIROC6',\n",
       " 'MRI-ESM2-0',\n",
       " 'SAM0-UNICON']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get names of all the models in the subset\n",
    "df_areacella = (df[((df.table_id == 'fx') \n",
    "                 & (df.variable_id == 'areacella')\n",
    "                 & (df.member_id == \"r1i1p1f1\"))])\n",
    "\n",
    "areacella_source_ids = []\n",
    "for name, group in df_areacella.groupby('source_id'):\n",
    "    if all([expt in group.experiment_id.values\n",
    "            for expt in experiment_ids1]):\n",
    "        areacella_source_ids.append(name)\n",
    "areacella_source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BCC-CSM2-MR',\n",
       " 'BCC-ESM1',\n",
       " 'CAMS-CSM1-0',\n",
       " 'CESM2',\n",
       " 'CESM2-WACCM',\n",
       " 'CanESM5',\n",
       " 'GFDL-ESM4',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'MIROC6',\n",
       " 'MRI-ESM2-0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tas_source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BCC-CSM2-MR',\n",
       " 'BCC-ESM1',\n",
       " 'CAMS-CSM1-0',\n",
       " 'CESM2',\n",
       " 'CESM2-WACCM',\n",
       " 'CanESM5',\n",
       " 'FGOALS-g3',\n",
       " 'GFDL-ESM4',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'MIROC6',\n",
       " 'MRI-ESM2-0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GFDL-ESM4',\n",
       " 'BCC-ESM1',\n",
       " 'BCC-CSM2-MR',\n",
       " 'MIROC6',\n",
       " 'MRI-ESM2-0',\n",
       " 'CESM2-WACCM',\n",
       " 'CESM2',\n",
       " 'CanESM5',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'CAMS-CSM1-0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids=list(set(tas_source_ids).intersection(pr_source_ids))\n",
    "source_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GFDL-ESM4',\n",
       " 'MRI-ESM2-0',\n",
       " 'MIROC6',\n",
       " 'CESM2-WACCM',\n",
       " 'CESM2',\n",
       " 'CanESM5',\n",
       " 'IPSL-CM6A-LR']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids_areacella=list(set(tas_source_ids).intersection(pr_source_ids).intersection(areacella_source_ids))\n",
    "source_ids_areacella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data: df_data has the catalogue of the variable of interest\n",
    "def load_data(df_data, source_id, expt_id):\n",
    "    \"\"\"\n",
    "    Load data for given variable, source and expt ids.\n",
    "    \"\"\"\n",
    "    uri = df_data[(df_data.source_id == source_id) &\n",
    "                  (df_data.experiment_id == expt_id)].zstore.values[0]\n",
    "    \n",
    "    gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get mean and variability and their changes\n",
    "def mean_dtas_calc(data_tmp,seas='all', weights_in=None):\n",
    "    # Set if season is 'all'\n",
    "    if seas==\"all\":\n",
    "        seas = ['DJF','MAM','JJA','SON']\n",
    "    \n",
    "    # Load\n",
    "    mu_hist = data_tmp['hist'].load()\n",
    "    mu_futr = data_tmp['futr'].load()\n",
    "    \n",
    "        \n",
    "    # Calculate mean of raw series\n",
    "    mu_hist = (data_tmp['hist'].sel(time=data_tmp['hist'].time.dt.season.isin(seas))\n",
    "               .tas.mean('time'))\n",
    "    mu_futr = (data_tmp['futr'].sel(time=data_tmp['futr'].time.dt.season.isin(seas))\n",
    "               .tas.mean('time'))\n",
    "    if weights_in is not None:\n",
    "        weights = weights_in.areacella\n",
    "        mu_histG = np.average(mu_hist,weights=weights)\n",
    "        mu_futrG = np.average(mu_futr,weights=weights)\n",
    "    else:\n",
    "        # Area weighting\n",
    "        latr = np.deg2rad(data_tmp['hist'].lat)\n",
    "        # Use the cosine of the converted latitudes as weights for the average\n",
    "        weights = np.cos(latr)\n",
    "        mu_histG = np.average(mu_hist.mean(\"lon\"),weights=weights)\n",
    "        mu_futrG = np.average(mu_futr.mean(\"lon\"),weights=weights)\n",
    "        \n",
    "    dtasG = mu_futrG-mu_histG\n",
    "#    dmu = mu_futr/mu_hist\n",
    "#    dmuG = np.average(dmu.mean(\"lon\"),weights=weights.values)\n",
    "\n",
    "    # Out \n",
    "    outp = xr.Dataset(\n",
    "        data_vars = {#'tas_hist': tas_hist,\n",
    "                     #'tas_futr': tas_futr,\n",
    "                     #'dmu':     dmu,\n",
    "                     'dtasG':    ([],dtasG)}\n",
    "\n",
    "        #coords={'lon':            (['lon'],results_tmp['hist'].lon),\n",
    "        #        'lat':             (['lat'],results_tmp['hist'].lat)}\n",
    "        )\n",
    "\n",
    "\n",
    "#     return(outp)\n",
    "    return(dtasG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0fd53131de4f0fa1036f684477d694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting GFDL-ESM4\n",
      "\n",
      "Begin processing time slice 1991-2020\n",
      " processed!\n",
      "1991-2020 processed!\n",
      "Begin processing time slice 2001-2030\n",
      " processed!\n",
      "2001-2030 processed!\n",
      "Begin processing time slice 2011-2040\n",
      " processed!\n",
      "2011-2040 processed!\n",
      "Begin processing time slice 2021-2050\n",
      " processed!\n",
      "2021-2050 processed!\n",
      "Begin processing time slice 2031-2060\n",
      " processed!\n",
      "2031-2060 processed!\n",
      "Begin processing time slice 2041-2070\n",
      " processed!\n",
      "2041-2070 processed!\n",
      "Begin processing time slice 2051-2080\n",
      " processed!\n",
      "2051-2080 processed!\n",
      "Begin processing time slice 2061-2090\n",
      " processed!\n",
      "2061-2090 processed!\n",
      "Begin processing time slice 2071-2100\n",
      " processed!\n",
      "2071-2100 processed!\n",
      "GFDL-ESM4 processed!\n",
      "\n",
      "\n",
      "Starting BCC-ESM1\n",
      "\n",
      "Begin processing time slice 1991-2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tdata = {}\n",
    "\n",
    "times = []\n",
    "for mod_name in tqdm(source_ids):\n",
    "    # get a 20 year period\n",
    "    # try:\n",
    "        print('\\n\\nStarting '+mod_name+'\\n')\n",
    "        ds_hist = load_data(df_mon_tas, mod_name, experiment_ids[0]).sel(time=slice('1976', '2005'))\n",
    "        ds_ssp  = load_data(df_mon_tas, mod_name, experiment_ids[1])\n",
    "        \n",
    "        try: # try getting areacella data\n",
    "            ds_areacella = load_data(df_areacella, mod_name, experiment_ids[0])\n",
    "\n",
    "        except: #if not available, flag it as 0. It will use np.cos(lat)\n",
    "            ds_areacella = None\n",
    "        \n",
    "        tdata[mod_name] = {}\n",
    "        \n",
    "        for time_slice in time_slices:\n",
    "            print('Begin processing time slice '+time_slice[0]+'-'+time_slice[1])\n",
    "            if ds_ssp.time.max().dt.year+1<int(time_slice[1]):\n",
    "                print(\"Future time series only goes until \"+str(ds_ssp.time.max().dt.year.values))\n",
    "                break\n",
    "            \n",
    "            # Get corresponding temporal slice of data and stage it\n",
    "            ds_ssp_tmp = ds_ssp.sel(time=slice(time_slice[0],time_slice[1]))\n",
    "            tdata[mod_name][\"t\"+time_slice[0]] = {'hist':ds_hist,'futr':ds_ssp_tmp}\n",
    "            \n",
    "            \n",
    "            dtasG = mean_dtas_calc(tdata[mod_name]['t'+time_slice[0]],\n",
    "                                                                   seas = 'all', weights_in=ds_areacella)\n",
    "                \n",
    "            times.append(xr.Dataset( {mod_name: ([ 'time'],  np.atleast_1d(dtasG))},\n",
    "                              coords = { 'time': ('time',[int(time_slice[0])])}))\n",
    "\n",
    "            print(' processed!')\n",
    "                \n",
    "            print(time_slice[0]+'-'+time_slice[1]+' processed!')\n",
    "            \n",
    "        print(mod_name+' processed!')\n",
    "    #except:\n",
    "    #    print(mod_name+\"broken\")\n",
    "    \n",
    "temp_da = xr.combine_by_coords(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_da.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_da.to_dataframe().to_csv('tresults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tresults_new=pd.read_csv('tresults.csv',index_col=0)\n",
    "tresults_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tresults_new)\n",
    "plt.title('temperature change for each model')\n",
    "plt.legend(tresults_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tresults_dict=tresults_new.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=np.zeros((len(time_slices),len(tresults_dict)))*np.nan \n",
    "modelcount=0\n",
    "modelnames=[]\n",
    "for model in tresults_dict:\n",
    "    modelnames.append(model)\n",
    "    timecount=0\n",
    "    for timeperiod in tresults_dict[model]:\n",
    "        dt[timecount,modelcount]=tresults_dict[model][timeperiod]\n",
    "        timecount=timecount+1\n",
    "    modelcount=modelcount+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRECIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get mean and variability and their changes\n",
    "def mean_var_calc(data_tmp,seas='all',weights_in=None):\n",
    "\n",
    "    # Set if season is 'all'\n",
    "    if seas==\"all\":\n",
    "        seas = ['DJF','MAM','JJA','SON']\n",
    "    \n",
    "    # Load\n",
    "    mu_hist = data_tmp['hist'].load()\n",
    "    mu_futr = data_tmp['futr'].load()\n",
    "    \n",
    "\n",
    "    # Calculate mean of raw series\n",
    "    mu_hist = (data_tmp['hist'].sel(time=data_tmp['hist'].time.dt.season.isin(seas))\n",
    "               .pr.mean('time'))\n",
    "    mu_futr = (data_tmp['futr'].sel(time=data_tmp['futr'].time.dt.season.isin(seas))\n",
    "               .pr.mean('time'))\n",
    "    \n",
    "    # Calculate standard deviation of detrended series\n",
    "    sd_hist = (xr.apply_ufunc(signal.detrend, data_tmp['hist'].fillna(0),\n",
    "                                    kwargs={'axis': 0}).where(~data_tmp['hist'].isnull())\n",
    "               .sel(time=data_tmp['hist'].time.dt.season.isin(seas))\n",
    "               .std(\"time\"))\n",
    "    sd_futr = (xr.apply_ufunc(signal.detrend, data_tmp['futr'].fillna(0),\n",
    "                                    kwargs={'axis': 0}).where(~data_tmp['futr'].isnull())\n",
    "               .sel(time=data_tmp['futr'].time.dt.season.isin(seas))\n",
    "               .std(\"time\"))\n",
    "\n",
    "    # Calculate variance\n",
    "    var_hist = sd_hist ** 2\n",
    "    var_futr = sd_futr ** 2\n",
    "    \n",
    "\n",
    "    # Area averaging #\n",
    "    if weights_in is not None:\n",
    "        print('Using areacella')\n",
    "        weights = weights_in.areacella\n",
    "        mu_histG = np.average(mu_hist,weights=weights)\n",
    "        mu_futrG = np.average(mu_futr,weights=weights)\n",
    "        var_histG = np.average(var_hist.pr,weights=weights)\n",
    "        var_futrG = np.average(var_futr.pr,weights=weights)\n",
    "    else:\n",
    "        print('Areacella unavailable; ')\n",
    "        ## if areacella is not available\n",
    "        ## Use the cosine of the converted latitudes as weights for the average\n",
    "        latr = np.deg2rad(data_tmp['hist'].lat)\n",
    "        weights = np.cos(latr)\n",
    "        mu_histG = np.average(mu_hist.mean(\"lon\"),weights=weights)\n",
    "        mu_futrG = np.average(mu_futr.mean(\"lon\"),weights=weights)\n",
    "        var_histG = np.average(var_hist.mean(\"lon\").pr,weights=weights)\n",
    "        var_futrG = np.average(var_futr.mean(\"lon\").pr,weights=weights)\n",
    "        \n",
    "    sd_histG = np.sqrt(var_histG)\n",
    "    sd_futrG = np.sqrt(var_futrG)\n",
    "    \n",
    "    # calculate changes\n",
    "    dmuG = mu_futrG/mu_histG\n",
    "#    dmu = mu_futr/mu_hist\n",
    "#    dmuG = np.average(dmu.mean(\"lon\"),weights=weights.values)\n",
    "    \n",
    "    dsd = sd_futr/sd_hist\n",
    "    dsdG = sd_futrG/sd_histG\n",
    "\n",
    "    # Out \n",
    "    dmuG = xr.DataArray(dmuG, coords=[], dims=[]).rename('dmuG')\n",
    "    dsdG = xr.DataArray(dsdG, coords=[], dims=[]).rename('dsdG')\n",
    "\n",
    "#     outp = xr.DataArray(\n",
    "#         data_vars = {'mu_hist': mu_hist,\n",
    "#                      'mu_futr': mu_futr,\n",
    "#                      #'dmu':     dmu,\n",
    "#                      'dmuG':    ([],dmuG),\n",
    "#                      'sd_hist': sd_hist.pr,\n",
    "#                      'sd_futr': sd_futr.pr,\n",
    "#                      #'dsd':     dsd.pr,\n",
    "#                      'dsdG':    ([],dsdG)},\n",
    "#         #coords={'lon':            (['lon'],results_tmp['hist'].lon),\n",
    "#         #        'lat':             (['lat'],results_tmp['hist'].lat)}\n",
    "#         )\n",
    "\n",
    "\n",
    "    return(mu_hist, mu_futr, dmuG,sd_hist.pr, sd_futr.pr, dsdG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = {}\n",
    "models = []\n",
    "\n",
    "for mod_name in tqdm(source_ids[0:2]):\n",
    "    # get a 20 year period\n",
    "    # try:\n",
    "        print('\\n\\nStarting '+mod_name+'\\n')\n",
    "        ds_hist = load_data(df_mon_pr, mod_name, experiment_ids[0]).sel(time=slice('1976', '2005'))\n",
    "        ds_ssp  = load_data(df_mon_pr, mod_name, experiment_ids[1])\n",
    "        try: # try getting areacella data\n",
    "            ds_areacella = load_data(df_areacella, mod_name, experiment_ids[0])\n",
    "\n",
    "        except: #if not available, flag it as 0. It will use np.cos(lat)\n",
    "            ds_areacella = None\n",
    "        \n",
    "        data[mod_name] = {}\n",
    "        \n",
    "        times = []\n",
    "        for time_slice in time_slices:\n",
    "            print('Begin processing time slice '+time_slice[0]+'-'+time_slice[1])\n",
    "            if ds_ssp.time.max().dt.year+1<int(time_slice[1]):\n",
    "                print(\"Future time series only goes until \"+str(ds_ssp.time.max().dt.year.values))\n",
    "                break\n",
    "            \n",
    "           # Get corresponding temporal slice of data and stage it\n",
    "            ds_ssp_tmp = ds_ssp.sel(time=slice(time_slice[0],time_slice[1]))\n",
    "            data[mod_name][\"t\"+time_slice[0]] = {'hist':ds_hist,'futr':ds_ssp_tmp}\n",
    "            \n",
    "            \n",
    "\n",
    "## implement this later, for now just do DJF \n",
    "            seasons_loop = []\n",
    "            for seas in seasons:\n",
    "                # Calculate means, sds,...\n",
    "                mu_hist, mu_futr, dmuG,sd_hist, sd_futr, dsdG= mean_var_calc(data[mod_name]['t'+time_slice[0]],\n",
    "                                                                           seas, weights_in = ds_areacella)\n",
    "                # concatenate dataarrays\n",
    "                das = [mu_hist.rename('mu_hist'),\n",
    "                       mu_futr.rename('mu_futr'),\n",
    "                       dmuG,\n",
    "                       sd_hist.rename('sd_hist'), \n",
    "                       sd_futr.rename('sd_futr'), dsdG]\n",
    "                das_expanded = [da.expand_dims(['model_name', 'time', 'season']) for da in das]\n",
    "                dasb = xr.merge(das_expanded)\n",
    "                dasb.coords['model_name'] = 'model_name', [mod_name]\n",
    "                dasb.coords['time'] = 'time', [int(time_slice[0])]\n",
    "                dasb.coords['season'] = 'season', [seas]\n",
    "                seasons_loop.append(dasb)\n",
    "                #print(seas+' processed!')\n",
    "            times.append(seasons_loop)    \n",
    "            print(time_slice[0]+'-'+time_slice[1]+' processed!')\n",
    "            \n",
    "        print(mod_name+' processed!')\n",
    "        models.append(times)\n",
    "    #except:\n",
    "    #    print(mod_name+\"broken\")\n",
    "    \n",
    "    \n",
    "\n",
    "temp_dapr = xr.combine_nested(models, concat_dim=['model_name','time', 'season'])\n",
    "temp_dapr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdG_djf = temp_dapr.sel(season='DJF')\n",
    "dsdG_djf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is rather brute force, sorry\n",
    "# dmuG, dsdG - also for different seasons \n",
    "dsdG_djf=results\n",
    "#dsdG_jja=results.copy()\n",
    "#dmuG_djf=results.copy()\n",
    "#dmuG_jja=results.copy()\n",
    "for model in results:\n",
    "    for timeperiod in results[model]:\n",
    "        dsdG_djf[model][timeperiod]=results[model][timeperiod]['DJF'].dsdG.values.item(0)\n",
    "#        dsdG_jja[model][timeperiod]=results[model][timeperiod]['JJA'].dsdG.values.item(0)\n",
    "#        dmuG_djf[model][timeperiod]=results[model][timeperiod]['DJF'].dmuG.values.item(0)\n",
    "#        dmuG_jja[model][timeperiod]=results[model][timeperiod]['JJA'].dmuG.values.item(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dsdG_djf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dsdG_djf).to_csv('dsdG_djf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdG_djf_new=pd.read_csv('dsdG_djf.csv',index_col=0)\n",
    "dsdG_djf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdG_djf=dsdG_djf_new.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdG_djf_np=np.zeros((len(time_slices),len(dsdG_djf)))*np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcount=0\n",
    "modelnames=[]\n",
    "for model in tresults_dict:\n",
    "    modelnames.append(model)\n",
    "    timecount=0\n",
    "    for timeperiod in dsdG_djf[model]:\n",
    "        dsdG_djf_np[timecount,modelcount]=dsdG_djf[model][timeperiod]\n",
    "        timecount=timecount+1\n",
    "    modelcount=modelcount+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdG_djf_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((dsdG_djf_np-1)*100)\n",
    "plt.title('DJF precip std change for each model')\n",
    "plt.legend(modelnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
